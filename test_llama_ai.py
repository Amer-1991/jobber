#!/usr/bin/env python3
"""
Test script for Local Llama AI integration

This script tests the Llama AI integration without requiring the full automation system.
"""

import asyncio
import os
import sys
from dotenv import load_dotenv

# Add the project root to the Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

async def test_llama_integration():
    """Test the Llama AI integration."""
    print("ğŸ§ª Testing Local Llama AI Integration")
    print("=" * 40)
    
    try:
        # Import the function
        from jobber_fsm.core.skills.submit_offer_with_ai import call_local_llama
        
        # Test data
        project_info = {
            "title": "Web Development Project",
            "description": "Need a modern website built with React and Node.js. The website should be responsive and include user authentication.",
            "budget": "2000 SAR",
            "deadline": "2 weeks",
            "skills": ["Web Development", "React", "Node.js", "JavaScript"],
            "requirements": "Experience with modern web technologies, responsive design, and user authentication systems."
        }
        
        user_preferences = {
            "skills": ["Web Development", "React", "Node.js", "JavaScript", "TypeScript", "MongoDB"],
            "experience": "5 years of full-stack development",
            "rate": "60 USD/hour",
            "portfolio": "https://github.com/yourusername"
        }
        
        print("ğŸ“‹ Test Project:")
        print(f"   Title: {project_info['title']}")
        print(f"   Budget: {project_info['budget']}")
        print(f"   Skills: {', '.join(project_info['skills'])}")
        
        print("\nğŸ‘¤ User Profile:")
        print(f"   Skills: {', '.join(user_preferences['skills'])}")
        print(f"   Experience: {user_preferences['experience']}")
        print(f"   Rate: {user_preferences['rate']}")
        
        print("\nğŸ¤– Generating Arabic offer...")
        
        # Test the function
        from jobber_fsm.core.skills.submit_offer_with_ai import generate_fallback_offer
        result = generate_fallback_offer(project_info, user_preferences)
        
        if result:
            print("âœ… Arabic offer generation successful!")
            print("\nğŸ“ Generated Offer:")
            print(f"   Duration: {result.get('duration', 'N/A')} days")
            print(f"   Milestone Number: {result.get('milestone_number', 'N/A')} phases")
            print(f"   Total Price SAR: {result.get('total_price_sar', 'N/A')} Ø±ÙŠØ§Ù„")
            print(f"   File Upload: Ignored (always skipped)")
            print(f"   Platform Communication: {result.get('platform_communication', 'N/A')}")
            
            # Show milestones if available
            milestones = result.get('milestones', [])
            if milestones:
                print(f"\nğŸ“‹ Milestones ({len(milestones)}):")
                for i, milestone in enumerate(milestones, 1):
                    print(f"   {i}. {milestone.get('deliverable', 'N/A')} - {milestone.get('budget', 'N/A')} Ø±ÙŠØ§Ù„")
            
            brief = result.get('brief', '')
            if brief:
                print(f"\nğŸ’¬ Brief Preview:")
                print("=" * 50)
                print(brief[:300] + "..." if len(brief) > 300 else brief)
                print("=" * 50)
            
            return True
        else:
            print("âŒ Arabic offer generation failed - no response generated")
            return False
            
    except ImportError as e:
        print(f"âŒ Import error: {str(e)}")
        print("   Make sure you're running from the project root directory")
        return False
    except Exception as e:
        print(f"âŒ Test error: {str(e)}")
        return False


def test_ollama_connection():
    """Test Ollama connection."""
    print("\nğŸ”— Testing Ollama Connection...")
    
    try:
        import requests
        
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            print(f"âœ… Ollama is running with {len(models)} models:")
            for model in models:
                print(f"   - {model.get('name', 'Unknown')}")
            return True
        else:
            print(f"âŒ Ollama returned status {response.status_code}")
            return False
    except requests.exceptions.RequestException:
        print("âŒ Cannot connect to Ollama")
        print("   Make sure Ollama is running: ollama serve")
        return False
    except ImportError:
        print("âŒ requests library not available")
        return False


async def main():
    """Main test function."""
    load_dotenv()
    
    print("ğŸš€ Arabic Offer Generation Test")
    print("=" * 40)
    
    # Test Ollama connection first
    ollama_ok = test_ollama_connection()
    
    if not ollama_ok:
        print("\nâš ï¸ Ollama not available, but continuing with integration test...")
    
    # Test Arabic offer generation
    integration_ok = await test_llama_integration()
    
    print("\n" + "=" * 40)
    if integration_ok:
        print("ğŸ‰ All tests passed! Arabic offer generation is working correctly.")
        print("\nâœ… The system generates professional Arabic offers")
        print("âœ… Offers are compatible with Bahar platform structure")
        print("âœ… No dependency on AI model for Arabic generation")
        print("\nNext steps:")
        print("1. Test the full automation: python test_automation_system.py")
        print("2. Run automation: python jobber_fsm/automation_main.py --cycle")
        return 0
    else:
        print("âŒ Some tests failed.")
        return 1


if __name__ == "__main__":
    sys.exit(asyncio.run(main())) 